"""Chat router for AI-powered meeting queries."""

import json
import logging
from typing import Optional

from fastapi import APIRouter, HTTPException
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field

from app.services.chat_service import ChatService
from app.services.llm.base import LLMProviderError


class MeetingChatRequest(BaseModel):
    """Request body for meeting-specific chat."""
    question: str = Field(..., min_length=1, description="The question to ask about the meeting")
    include_related: bool = Field(False, description="Whether to include context from related meetings")


class OverallChatRequest(BaseModel):
    """Request body for overall chat across all meetings."""
    question: str = Field(..., min_length=1, description="The question to ask about meetings")
    max_meetings: int = Field(5, ge=1, le=20, description="Maximum number of meetings to include")
    include_transcripts: bool = Field(True, description="Whether to include full transcripts")


def create_chat_router(chat_service: ChatService) -> APIRouter:
    """Create the chat router with API endpoints.
    
    Args:
        chat_service: The chat service instance
        
    Returns:
        FastAPI router with chat endpoints
    """
    router = APIRouter()
    logger = logging.getLogger("notetaker.api.chat")

    @router.post("/api/chat/meeting/{meeting_id}")
    def chat_meeting(meeting_id: str, payload: MeetingChatRequest):
        """Chat about a specific meeting.
        
        Returns an SSE stream with tokens as they are generated by the LLM.
        Each line is in the format: data: {"token": "..."}\n\n
        Stream ends with: data: [DONE]\n\n
        """
        logger.info(
            "Chat meeting request: meeting_id=%s question='%s' include_related=%s",
            meeting_id, payload.question[:50], payload.include_related
        )
        
        def generate():
            try:
                for token in chat_service.chat_meeting(
                    meeting_id=meeting_id,
                    question=payload.question,
                    include_related=payload.include_related,
                ):
                    yield f"data: {json.dumps({'token': token})}\n\n"
            except LLMProviderError as exc:
                logger.warning("Chat meeting failed: %s", exc)
                yield f"data: {json.dumps({'error': str(exc)})}\n\n"
            except Exception as exc:
                logger.exception("Chat meeting error: %s", exc)
                yield f"data: {json.dumps({'error': 'Chat failed'})}\n\n"
            finally:
                yield "data: [DONE]\n\n"
        
        return StreamingResponse(
            generate(),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "X-Accel-Buffering": "no",
            }
        )

    @router.post("/api/chat/overall")
    def chat_overall(payload: OverallChatRequest):
        """Chat across all meetings.
        
        Uses hybrid search to find relevant meetings, then queries the LLM.
        Returns an SSE stream with tokens as they are generated.
        Each line is in the format: data: {"token": "..."}\n\n
        Stream ends with: data: [DONE]\n\n
        """
        logger.info(
            "Chat overall request: question='%s' max_meetings=%d include_transcripts=%s",
            payload.question[:50], payload.max_meetings, payload.include_transcripts
        )
        
        def generate():
            try:
                for token in chat_service.chat_overall(
                    question=payload.question,
                    max_meetings=payload.max_meetings,
                    include_transcripts=payload.include_transcripts,
                ):
                    yield f"data: {json.dumps({'token': token})}\n\n"
            except LLMProviderError as exc:
                logger.warning("Chat overall failed: %s", exc)
                yield f"data: {json.dumps({'error': str(exc)})}\n\n"
            except Exception as exc:
                logger.exception("Chat overall error: %s", exc)
                yield f"data: {json.dumps({'error': 'Chat failed'})}\n\n"
            finally:
                yield "data: [DONE]\n\n"
        
        return StreamingResponse(
            generate(),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "X-Accel-Buffering": "no",
            }
        )

    @router.post("/api/chat/meeting/{meeting_id}/sync")
    def chat_meeting_sync(meeting_id: str, payload: MeetingChatRequest) -> dict:
        """Non-streaming chat about a specific meeting.
        
        Returns the complete response as JSON.
        """
        logger.info(
            "Chat meeting sync: meeting_id=%s question='%s'",
            meeting_id, payload.question[:50]
        )
        
        try:
            response = chat_service.chat_meeting_sync(
                meeting_id=meeting_id,
                question=payload.question,
                include_related=payload.include_related,
            )
            return {"response": response}
        except LLMProviderError as exc:
            logger.warning("Chat meeting sync failed: %s", exc)
            raise HTTPException(status_code=400, detail=str(exc)) from exc
        except Exception as exc:
            logger.exception("Chat meeting sync error: %s", exc)
            raise HTTPException(status_code=500, detail="Chat failed") from exc

    @router.post("/api/chat/overall/sync")
    def chat_overall_sync(payload: OverallChatRequest) -> dict:
        """Non-streaming chat across all meetings.
        
        Returns the complete response as JSON.
        """
        logger.info(
            "Chat overall sync: question='%s' max_meetings=%d",
            payload.question[:50], payload.max_meetings
        )
        
        try:
            response = chat_service.chat_overall_sync(
                question=payload.question,
                max_meetings=payload.max_meetings,
                include_transcripts=payload.include_transcripts,
            )
            return {"response": response}
        except LLMProviderError as exc:
            logger.warning("Chat overall sync failed: %s", exc)
            raise HTTPException(status_code=400, detail=str(exc)) from exc
        except Exception as exc:
            logger.exception("Chat overall sync error: %s", exc)
            raise HTTPException(status_code=500, detail="Chat failed") from exc

    return router
