for each item here, identify if it's a bug or a new feature. if it's a new feature, assess whether it's large and has multiple open questions about it.  if it is large, follow the @thinking/from-thinking-to-coding/ methodology to understand the feature and implement it in phases.

add pointers to specs, implementation plans and any other documentation if any are created for any of the items.

update the status of each of the items below.  mrak them as "in progress" once you start working on it.  mark it as "implemented" when it has been impelemented in code, but not yet confirmed to be fully working.

when it's fully working, move it to a todo-done.md file

---

in the meeting page, after i hit stop, every approx 5 seconds, i see "loading meeting" flash on the upper left.  explain why that is.  don't make a change.  just explain

---

# redesign the meeting page.  do not make functional changes.
  - Type: UI redesign
  - Status: done

----

# redesign settings page to be more vertical space efficient
  - Type: UI redesign
  - Status: in progress
  - Plan: .cursor/plans/settings_page_redesign_cae1ba7c.plan.md

# final diarization
  - Type: new feature
  - Status: done

Implemented LLM-based speaker identification and enhanced finalization pipeline with status updates.

----
# manual summarization

done!

let's take a more manual approach to summarization until the transcription is done.  

remove all the controls in the summarization debug section at the bottom of the meetings page, and remove the current auto summarization code.  keep all the transcription code.  only remove the summarization of the transcription.


have these different text buffers:  
1: transcription
2: user notes
3: summary

in the summary debug section at the bottom of the menu page, have a text box for each text buffer.

fill the transcription text buffer as the transcription text comes in.

have a summarize button that i can press.  when i press it, ask the llm to summarize the transcription so far.  

put the llm prompt that you are using to ask the llm to summarize the transcription in a separate file.  load from that file at runtime so that i can see it and iterate on the prompt myself.  


---

# Bug 2: Hard to stop transcription
  - Type: bug
  - Status: done
  - Plan: docs/plans/responsive-audio-architecture.md

When hitting stop, transcription continues for 60+ seconds before actually stopping.

**Root cause:** Whisper processes audio in fixed 30-second windows due to its transformer architecture. The encoder runs on the full 30-second spectrogram regardless of actual content length (smaller inputs are padded with silence). The cancel_event was only checked between encoder passes, not during file reading.

**Solution implemented:**

1. **Chunked file transcription** (`chunked_transcribe_and_format`):
   - Audio files are now read in model-optimal chunks (30s for Whisper)
   - Cancellation is checked BEFORE reading each new chunk
   - When stop is pressed, file reading stops immediately
   - Only the current chunk continues through transcription
   - Already-transcribed segments are preserved

2. **Live audio capture improvements**:
   - Added `signal_capture_stopped()` and `drain_live_queue()` to AudioCaptureService
   - When stop is pressed, audio capture stops immediately
   - Remaining buffered audio is drained and sent as final chunk
   - Live transcription stream aborted first, then recording stopped

3. **UI improvements**:
   - Summary timer and countdown stop immediately when stop is pressed
   - Clear status messages: "Audio capture stopped. Finishing transcription..."
   - Polling continues in background until transcription completes
   - Different messages for file vs live transcription

**Files changed:**
- `app/services/audio_capture.py` - Added drain_live_queue(), signal_capture_stopped(), is_capture_stopped()
- `app/services/transcription/base.py` - Added get_chunk_size() to TranscriptionProvider
- `app/services/transcription/whisper_local.py` - Implemented get_chunk_size() returning 30.0
- `app/services/transcription_pipeline.py` - Added chunked_transcribe_and_format() and get_chunk_size()
- `app/services/live_transcription.py` - New decoupled live transcription service
- `app/routers/transcription.py` - Updated _run_simulated_transcription to use chunked method
- `app/static/app.js` - Updated stopRecording() and stopFileRecording() for responsive UI
- `app/static/meeting.js` - Updated stopTranscription() to stop timer immediately

---

# Bug 3: No attendees detected (diarization not working)
  - Type: bug
  - Status: diagnosed
  - Plan: docs/plans/bug-fix-plan-2026-02-10.md#bug-3-no-attendees-detected-diarization-not-working

No attendees appearing in attendees list. Speakers are all null in transcript segments.

**Root cause:** HuggingFace pyannote model license not accepted. The model requires visiting https://hf.co/pyannote/speaker-diarization-3.1 and accepting terms.

**Evidence (from logs):**
```
Could not download 'pyannote/speaker-diarization-3.1' pipeline.
visit https://hf.co/pyannote/speaker-diarization-3.1 to accept the user conditions.
Diarization failed: WhisperX diarization failed
```

---


# update to attendee list
  - Type: new feature
  - Size: medium
  - Status: implemented

update the attendees ui to make it easier to identify and update the attendees.  the ui should show a list of each attendee.  when i select any person, there should be a text box on the right side that shows what the person has said so far to help me know who it is.

i should be able to rename the person selected.  give me a rename button that puts it in rename mode that does rename in place, similar to how file renames work.   

give me a button that does auto-rename.  this should prompt the ai to identify the name of the person based upon the conversation content.

  Implementation notes:
  - Replaced textarea with interactive attendee list (left column)
  - Selecting an attendee shows their spoken segments (right column)
  - Rename button enables inline editing (Enter to save, Escape to cancel)
  - Auto button calls AI to identify speaker name from their speech
  - Backend API: POST /api/meetings/{id}/attendees/{id}/auto-rename
  - Added `prompt()` method to all LLM providers for raw prompts

---

# auto-detect attendees
  - Type: new feature
  - Size: small
  - Status: pending

in the meetings page for attendees, the system should detect the people in the call.  if there's a new person detected, add that person in this section.  

---

# user login
  - Type: new feature
  - Size: large (multi-step, open questions: auth provider, scopes, privacy, multi-user storage layout)
  - Status: deferred
  - Docs: docs/specs/user-login-spec.md, docs/plans/user-login-plan.md

create a user login.  can we connect this to google identiy login?  let the user profile be connected to google and also get access to the user's google drive?

change the file storage to be structured for multiple users and load the meeting configurations and the settings to be per user.  make it so that each person's login get their own meetings and settings

---

# e2e test harness
  - Type: new feature
  - Size: medium
  - Status: implemented
  - Docs: docs/specs/e2e-test-harness-spec.md, docs/plans/e2e-test-harness-plan.md

end-to-end test harness for automated testing of in-progress features.
trigger via URL: /test?suite=<id> or /test?all=true
outputs detailed logs to logs/test_*.log
