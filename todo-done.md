# Completed Features

Items moved here from todo when fully working.

---

## auto meeting title generation
  - Type: new feature
  - Size: medium
  - Status: done
  - Docs: docs/specs/auto-meeting-title-spec.md, docs/plans/auto-meeting-title-plan.md

the meeting title should be automatically generated by the content unless the user manually sets it.   it should be generated sometime early in the meeting as a draft.  then when the meeting is completed and the summarization has happened, it should be updated with a more final one.  but if the user manually sets the title at any point, that should be kept

---

## smart real time summary parsing
  - Type: new feature
  - Size: large (multi-step, open questions: data model, prompts, UI, storage)
  - Status: done
  - Docs: docs/specs/smart-real-time-summary-spec.md, docs/plans/smart-real-time-summary-plan.md

every 30 seconds, we should prompt the llm to summarize the conversation so far.  usually a conversation will have multiple points of discussioin that flow from one to another.   to make the summary parsing stable notetaker should keep track of a what is "summarized" and what is "in progresss".   "in progress" is the current topic being discussed, and who's summary is still evolving.  

in any live conversation, there should be 3 categories of transcribed text that notetaker manages.  1) done.  2) draft.  3) streaming.   there should also be 2 categories of summarized text that the notetaker manages: 1) summarized, 2) interim.

here's the workflow:

1) all new live transcriptions should get added to "streaming".  
2) every 30 seconds,  notetaker should take as much content from "streaming" as possible starting from the top and going as far down as posible, but only whole sentences.  do not cut off sentences.  
3) since this is a live transcription, it will have some errors.  it should first send it to the llm with a well crafted prompt that explains how transcriptions have errors, and the typical type of errors that voice transcriptions have.  it should process the transcribed text and use the context of the conversation available for it to clean up the transcription and try to make it more likely to be correct to what the people actually said. 
4) it should then append it to the previously transcribed text in "draft".
5) it should pass the draft to the llm and ask the llm to identify the number of different topics that are discussed in the draft.  it should create a summary of each of the topics it identified.  starting from the first topic, if that topic has been fully discussed, it should append the summarization of that topic to the "summarized" cateogry and remove it from "interim". it should append the corresponding transcript to the "done" category and remove it from "draft".  
6) do this for each topic in draft except the last topic of draft which is still being discussed.  

---

## smart real time summary debug ui
  - Type: new feature
  - Size: small
  - Status: done
  - Docs: docs/specs/smart-real-time-summary-spec.md, docs/plans/smart-real-time-summary-plan.md

in the meeting ui, add a debug button at the bottom.  when the user presses the debug button, show two columns of text boxes that show all of these content streams as they are being updated so that we can debug it. on the left have the "done" "draft" and "streaming" text.   on the right hav ethe "summarized" and "interim" content.  these should be consistently updating to the current contents. 

these should constantly be scrolled to the bottom.

---

## cleanup main window
  - Type: new feature
  - Size: medium
  - Status: done
  - Docs: docs/specs/cleanup-main-window-spec.md, docs/plans/cleanup-main-window-plan.md

move the 3 transcript settings to the settings page

move the settings button to the upper right.  put it by a profile dropdown that lets the user login, logout and configure settings.

change the start and stop recording buttons to a single button that toggles recording and shows the current state of recording

have a meetings list that includes all the prior meetings in chronological order (most recent at the top).  if th4ere's one in progress it should be at the top and marked as "in progress"

move the delete, exprot to markdown, ability to change title and all the meeting specific changing controls into the meetings page.

on the meeting list, make a click on the meeting open the meeting page for the single meeting.

---

## meeting mode
  - Type: new feature
  - Size: large (multi-step, open questions: live summary cadence, in-progress transcript refresh rules, meeting status definition)
  - Status: done
  - Docs: docs/specs/meeting-mode-spec.md, docs/plans/meeting-mode-plan.md

create a meeting view page.  this should be a dedicated web page - one per meeting.  this should be able to handle existing meetings in progress as well as meetings that have concluded.

on the left side there should be a meeting summary, which is an ai summary of the meeting.  if the meeting is in progress, it should create a running summary of the conversation. 

the right side should have the full trascript (or transcript in progress if the meeting is still going)

it should have all the meeting attendees at the top

---

## settings page
  - Type: new feature
  - Size: medium
  - Status: done
  - Docs: docs/specs/model-chooser-settings-spec.md

create a settings page that lets the user configure different necessary settings.  
move all of the settings that are currently in the main page into this settings page.

---

## ai model specification
  - Type: new feature
  - Size: large
  - Status: done
  - Docs: docs/specs/model-chooser-settings-spec.md

add the ability for users to set the api key or lm studio local server to know what ai model to use.  
enumerate all the available models.  let the user pick which model they want.  accept api keys from any of the frontier model providers.

---

## set live transcript as default "on"
  - Type: new feature
  - Size: small
  - Status: done
  - Docs: none

---

## diarization with whisperx
  - Type: new feature
  - Size: large (new dependency + diarization pipeline swap)
  - Status: done
  - Docs: docs/specs/diarization-whisperx-spec.md, docs/plans/diarization-whisperx-plan.md

update the install script to properly install it.
implement the diarization to identify different speakers

Implementation notes:
- WhisperX and pyannote providers implemented for batch/file diarization
- Diart provider added for real-time diarization during live recording
- Unified TranscriptionPipeline centralizes segment formatting, diarization, summarization
- All transcription flows (file, streaming, simulated, live) now share common pipeline
- Real-time diarization activates when provider=diart is configured
- Requires accepting HuggingFace licenses for pyannote models

Commits:
- f2b4c02: Fix diarization in simulated transcription, improve HF error messages
- ab0a105: Unify transcription code flows with TranscriptionPipeline
- 4b70efa: Add real-time speaker diarization with Diart

---

## consolidate transcript chunks
  - Type: new feature
  - Status: done
  - Plan: .cursor/plans/transcript_chunk_consolidation_9d374512.plan.md

Merges consecutive transcript segments from the same speaker into larger chunks (configurable, default 15 seconds max). Settings in Transcription section. Debug toggle to view raw segments.

Fixed: Raw segments toggle now works correctly. Also fixed scroll position in raw transcript textbox to not auto-scroll when user has scrolled up.

---

## redesign the meeting page
  - Type: UI redesign
  - Status: done

---

## redesign settings page to be more vertical space efficient
  - Type: UI redesign
  - Status: done
  - Plan: .cursor/plans/settings_page_redesign_cae1ba7c.plan.md

---

## final diarization
  - Type: new feature
  - Status: done

Implemented LLM-based speaker identification and enhanced finalization pipeline with status updates.

---

## manual summarization
  - Type: new feature
  - Status: done

Manual approach to summarization with three text buffers (transcription, user notes, summary). Summarize button prompts LLM to summarize transcription. LLM prompt stored in separate file for iteration.

---

## Bug 2: Hard to stop transcription
  - Type: bug
  - Status: done
  - Plan: docs/plans/responsive-audio-architecture.md

When hitting stop, transcription continues for 60+ seconds before actually stopping.

**Root cause:** Whisper processes audio in fixed 30-second windows due to its transformer architecture. The encoder runs on the full 30-second spectrogram regardless of actual content length (smaller inputs are padded with silence). The cancel_event was only checked between encoder passes, not during file reading.

**Solution implemented:**

1. **Chunked file transcription** (`chunked_transcribe_and_format`):
   - Audio files are now read in model-optimal chunks (30s for Whisper)
   - Cancellation is checked BEFORE reading each new chunk
   - When stop is pressed, file reading stops immediately
   - Only the current chunk continues through transcription
   - Already-transcribed segments are preserved

2. **Live audio capture improvements**:
   - Added `signal_capture_stopped()` and `drain_live_queue()` to AudioCaptureService
   - When stop is pressed, audio capture stops immediately
   - Remaining buffered audio is drained and sent as final chunk
   - Live transcription stream aborted first, then recording stopped

3. **UI improvements**:
   - Summary timer and countdown stop immediately when stop is pressed
   - Clear status messages: "Audio capture stopped. Finishing transcription..."
   - Polling continues in background until transcription completes
   - Different messages for file vs live transcription

**Files changed:**
- `app/services/audio_capture.py` - Added drain_live_queue(), signal_capture_stopped(), is_capture_stopped()
- `app/services/transcription/base.py` - Added get_chunk_size() to TranscriptionProvider
- `app/services/transcription/whisper_local.py` - Implemented get_chunk_size() returning 30.0
- `app/services/transcription_pipeline.py` - Added chunked_transcribe_and_format() and get_chunk_size()
- `app/services/live_transcription.py` - New decoupled live transcription service
- `app/routers/transcription.py` - Updated _run_simulated_transcription to use chunked method
- `app/static/app.js` - Updated stopRecording() and stopFileRecording() for responsive UI
- `app/static/meeting.js` - Updated stopTranscription() to stop timer immediately

---

## redesign the meetings page UI
  - Type: UI redesign
  - Status: done

1) remove the extra vertical blank space at the top of the page
2) move the title of the meeting to the header row
3) make the page fill the horizontal width of the window
4) move the attendees list to a left panel list with popover on select
5) summary, transcript, notes and ai chat in a 2x2 grid; debug panel as right sidebar toggle

---

## meeting ai query
  - Type: new feature
  - Size: medium
  - Status: done

Provide a chat interface in the meeting page where the user can ask questions about a specific meeting. The LLM answers primarily about that meeting but has access to other meetings as background context.

  Implementation notes:
  - Chat UI added to meeting page below Notes section
  - API: POST /api/chat/meeting/{meeting_id} (SSE streaming)
  - Prompt template: app/prompts/meeting_chat_prompt.txt
  - Supports optional include_related flag to search other meetings for context

---

## overall ai query
  - Type: new feature
  - Size: medium
  - Status: done

Provide a chat interface in the main home page where the user can ask questions about the aggregate of all meetings. Uses hybrid search to avoid sending all full transcripts.

  Implementation notes:
  - Chat UI added to home page after Meetings section
  - API: POST /api/chat/overall (SSE streaming)
  - Prompt template: app/prompts/overall_chat_prompt.txt
  - Uses hybrid search: keyword search on titles/summaries, load full transcripts for top 5 matches
  - SearchService (app/services/search_service.py) handles meeting search

---

## dark mode
  - Type: new feature
  - Size: small
  - Status: done

Dark mode with light/dark/system toggle in settings page. Persisted to config.json. Applied across all pages. Icons adapt to mode.

---

## user notes feature
  - Type: new feature
  - Size: medium
  - Status: done
  - Plan: .cursor/plans/user_notes_feature_fef31a46.plan.md

Enable users to type timestamped notes during meetings. Notes are logged with the time the user started typing, acknowledging natural lag between discussion and note-taking.

Implementation notes:
- Notes panel in meeting page lower-right with running log of submitted notes
- Timestamp captured when user starts typing; persists until explicit clear
- Submit button adds note to log; clear button (X) clears input and timestamp
- Inline editing of existing notes preserves original timestamp
- Post-meeting notes marked as "Added after meeting"
- Draft auto-save to meeting JSON with restore on return
- Backend API: GET/POST/PATCH/DELETE for notes CRUD, PUT for draft
- Summarization includes notes with timestamp context in LLM prompt
- Chat integration: notes included in meeting and overall chat prompts

---

## ai chat issues
  - Type: bug / enhancement
  - Status: done
  - Plan: .cursor/plans/ai_chat_issues_implementation_03a3c2c7.plan.md

### 1) chat persistence

Chat messages (user prompts and LLM responses) are preserved and restored on reload. Each message has a timestamp. Meeting chats stored in meeting JSON; homepage chat in homepage_state.json.

Implementation notes:
- Backend: 4 new API endpoints in chat.py (GET/PUT for meeting and homepage chat history)
- MeetingStore: get_chat_history() / save_chat_history() read/write chat_history in meeting JSON
- ChatService: get_homepage_chat_history() / save_homepage_chat_history() read/write data/homepage_state.json
- Frontend: ChatUI accepts historyEndpoint option; loadHistory() on init, saveHistory() after each send/response cycle and clear
- Messages stored as {role, content, timestamp} with ISO timestamps displayed as relative/short datetime

### 2) chat search

Search box in chat panel banner with case-insensitive search, next/prev navigation, and keyboard shortcuts.

Implementation notes:
- Search bar added to all ChatUI variants (fullscreen, minimal, standard) in chat.js
- Ctrl+F / Cmd+F opens search when chat panel is focused
- Case-insensitive search with <mark> highlighting across all .chat-message-content elements
- Next/Prev navigation with wrapping, Enter/Shift+Enter shortcuts, Escape to close
- Count display shows "N/M" (current match / total matches)

### 3) attendee names in chat context

LLM knows about renamed attendees in both meeting and homepage chat contexts.

Implementation notes:
- Added _format_transcript_with_speakers() and _format_attendee_list() helpers to ChatService
- All transcript formatting locations resolve speaker_id to attendee name using the meeting's attendees list
- Attendee list included in prompt context so LLM knows meeting participants

---

## update to attendee list
  - Type: new feature
  - Size: medium
  - Status: done

Interactive attendee list UI with selection, inline rename, and AI auto-rename capabilities.

Implementation notes:
- Replaced textarea with interactive attendee list (left column)
- Selecting an attendee shows their spoken segments (right column)
- Rename button enables inline editing (Enter to save, Escape to cancel)
- Auto button calls AI to identify speaker name from their speech
- Backend API: POST /api/meetings/{id}/attendees/{id}/auto-rename
- Added `prompt()` method to all LLM providers for raw prompts
